---
layout: single
title:  "[ML] Transfer Learning"
excerpt: "Transfer Learning 이란"

categories:
  - Machine Learning, Python

tags:
  - [Blog, jekyll, Github, Git]

sidebar:
  nav: "docs"

toc: true
toc_label: "목록"
toc_icon: "bars"
toc_sticky: true
 
date: 2022-04-09
last_modified_at: 2022-04-09
---

<!-- ## Transfer Learning vs. Fine Tuning -->

## Transfer Learning

* 전이 학습

* 사전 훈련된 모델(pre-trained model)을 새로운 작업에 대한 모델의 시작점으로 재사용하는 Machine Learning method

  > *기존의 머신 러닝 모델은 처음부터 학습해야 함* -> 계산 비용이 많이 들고 고성능을 달성하기 위해 많은 양의 데이터가 필요
  >
  > *전이 학습*은 계산적으로 효율적이며 작은 데이터 세트를 사용하여 더 나은 결과를 얻는 데 도움
  >
  > ![616b35e3dcd432047dd02ea5_uYLdnVpAfjC3DC7eWJM2xWyQin_dbVcak0JlRpd7S2bAkdylh-9JITWttww3Wq8fKI56Tl3_v7Y-aVh4nKgl4mZl4ZvcoUIViQRJhBBSw2cpC087oc2iZYvBytr8o1ks1FY1LQxh=s0.png (919×495)](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/616b35e3dcd432047dd02ea5_uYLdnVpAfjC3DC7eWJM2xWyQin_dbVcak0JlRpd7S2bAkdylh-9JITWttww3Wq8fKI56Tl3_v7Y-aVh4nKgl4mZl4ZvcoUIViQRJhBBSw2cpC087oc2iZYvBytr8o1ks1FY1LQxh%3Ds0.png)

* 새로운 작업에 전이 학습을 적용하면 적은 양의 데이터로 훈련하는 것보다 훨씬 더 높은 성능을 얻을 수 있다.

* ImageNet, AlexNet, Inception은 Transfer learning을 기반으로 하는 모델의 대표적인 예

  ![img](http://www.aritrasen.com/wp-content/uploads/2019/05/1_qfQ3hmHLwApXZBN-A85r8g.png)

그림 출처 : https://www.aritrasen.com/deep-learning-with-pytorch-cnn-transfer-learning-2-2/



## 단어

* pre-trained model : 내가 풀고자 하는 문제와 비슷하면서 사이즈가 큰 데이터로 이미 학습이 되어 있는 모델입니다. 그런 큰 데이터로 모델을 학습시키는 것은 오랜 시간과 연산량이 필요하므로, 관례적으로는 이미 공개되어있는 모델들을 그저 import해서 사용 (ex. [VGG](https://arxiv.org/pdf/1409.1556.pdf), [Inception](https://arxiv.org/pdf/1512.00567.pdf), [MobileNet](https://arxiv.org/pdf/1704.04861.pdf))
* 

## References

* https://jeinalog.tistory.com/13
* https://newindow.tistory.com/254
* https://velog.io/@hyangki0119/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%A0%84%EC%9D%B4%ED%95%99%EC%8A%B5Transfer-learning%EA%B3%BC-%ED%8C%8C%EC%9D%B8-%ED%8A%9C%EB%8B%9DFine-tuning%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90
* https://www.v7labs.com/blog/transfer-learning-guide#what-is-transfer-learning
* 