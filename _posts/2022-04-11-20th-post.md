---
layout: single
title:  "[ML] Naive Bayes classifier"
excerpt: "based on Bayesian Estimation"

categories:
  - Deep Learning, Machine Learning, Python

tags:
  - [Blog, jekyll, Github, Git]

sidebar:
  nav: "docs"

toc: true
toc_label: "목록"
toc_icon: "bars"
toc_sticky: true
 
date: 2022-04-11
last_modified_at: 2022-04-11
---

Naive Bayes classifier을 알기위해서는 우선 베이지안 추정(Bayesian Estimation)을 알아야 한다.

## Bayesian Estimation

추론 대상의 사전 확률과 추가적인 정보를 기반으로 해당 대상의 사후 확률을 추론하는 통계적 방법

*베이즈 추정법을 사용하는 이유는 추정된 모숫값 숫자 하나만으로는 추정의 신뢰도와 신뢰구간을 구할 수 없기 때문이다.*

> e.g) 어떤 마을 전체 사람들의 10.5%가 암 환자이고, 89.5%가 암 환자가 아닙니다. 이 마을의 모든 사람에 대해 암 검진을 실시했다고 합시다. 암 검진시 양성 판정, 음성 판정 결과가 나올 수 있습니다. 하지만 검진이 100% 정확하지는 않고 약간의 오차가 있습니다. 암 환자 중 양성 판정을 받은 비율은 90.5%, 암 환자 중 음성 판정을 받은 비율은 9.5%, 암 환자가 아닌 사람 중 양성 판정을 받은 비율은 20.4%, 암 환자가 아닌 사람 중 음성 판정을 받은 비율은 79.6%입니다. 어떤 사람이 양성 판정을 받았을 때 이 사람이 암 환자일 확률은 얼마일까요?

![img](https://miro.medium.com/max/1200/1*aFhOj7TdBIZir4keHMgHOw.png)

> - C: Cancer(암 환자), P: Positive(양성), N: Negative(음성)
>   P(C): 암 환자일 확률 = 0.105
>   P(~C): 암 환자가 아닐 확률 = 0.895
>   P(P|C): 암 환자일 때 양성 판정을 받을 확률 = 0.905 (민감도, sensitivity)
>   P(N|C): 암 환자일 때 음성 판정을 받을 확률 = 0.095
>   P(P|~C): 암 환자가 아닐 때 양성 판정을 받을 확률 = 0.204
>   P(N|~C): 암 환자가 아닐 때 음성 판정을 받을 확률 = 0.796 (특이도, specificity)
>
> 이때 P(C|P): 어떤 사람이 양성 판정을 받았을 때 이 사람이 암 환자일 확률은?
>
> 베이즈의 추정에 의해
>
> P(C|P) = P(P|C)*P(C) / P(P)
>
> 여기서, P(P) = P(P, C) + P(P, ~C) = P(P|C)*P(C) + P(P|~C)*P(~C)
>
> * P(A, B) : A와 B가 동시에 발생할 확률
>
> P(P), 즉 양성 판정을 받을 확률은 암 환자이자 양성 판정을 받을 확률(P(P, C))과 암 환자가 아닌데 양성 판정을 받을 확률(P(P, ~C))의 합과 같다.
>
> *조건부 확률*에 의해 P(P, C) = P(P|C)*P(C)이고, P(P, ~C) = P(P|~C)*P(~C)
>
> 따라서, P(C|P) = P(P|C)*P(C) / P(P) = 0.905*0.105 / (0.905*0.105 + 0.204*0.895) = 0.342
>
> => 암 환자는 양성 판정을 받을 확률이 높다. 하지만 양성 판정을 받았어도 암이 아닐 확률이 더 높다는 걸 알 수 있다.

#### 조건부 확률

어떤 사건 B가 일어났을 때 사건 A가 발생할 **확률**

<center><img src="C:\Users\INNO-2\AppData\Roaming\Typora\typora-user-images\image-20220411132212086.png" alt="image-20220411132212086" style="zoom: 50%;" /></center>

## Naive Bayes classifier

naive : 순진한

나이브 베이즈 분류는 베이즈 정리에 기반한 통계적 분류 기법(조건부 확률 모델)이다. 가장 단순한 지도 학습 (supervised learning) 중 하나로 빠르고, 정확하며, 믿을만한 알고리즘이다. 정확성도 높고 대용량 데이터에 대해 속도도 빠릅니다.

<span style='background-color: #fff5b1'>나이브 베이즈는 **feature끼리 서로 독립**이라는 조건이 필요하다.</span>

> e.g ) 날씨 정보와 축구 경기 여부에 대한 데이터, 날씨에 대한 정보를 기반으로 축구를 할것인지 안 할 것인지 확률을 구하는 예제



*Bayes Inference에서 처럼 특정 확률 값을 구하는 문제가 아니고, 가장 높은 확률 값을 제공하는 분류값(class)를 찾는 것이 주 목적입니다.*

특정 feature X(vector)에 대하여 P(y)를 최대화 시키는 y 값(class/lable)을 찾는 것은 이는 분류 target인 여러 y(class, lable)값, 가장 높은 확률을 보이는 y 값으로 분류한다는 뜻으로 해석할 수 있습니다.

> e.g ) Spam 메일을 구분
>
> 각 단어별로 Spam 메일이라고 분류된 메일에서 전체 등장한 횟수를 다 세어서 P(해당 단어) = (해당 단어 등장 횟수) / (전체 단어 수) 와 같은 frequency counting으로 Spam Distribution
>
> <center><img src="https://mblogthumb-phinf.pstatic.net/20160709_161/kenshinhm_1468065136052PdWiI_PNG/image_2957267981468065124001.png?type=w800" alt="img" style="zoom:67%;" />



## Words

- **모집단 :** 통계적 관찰이 되는 집단.

- **전수조사 :** 모집단의 어떤 한부분이나 샘플(표본)을 조사하는 것이아니라 모집단 전체를 대상으로 조사하는 것.

- **모수(μ)** : 모수는 모집단의 특성(모평균,모분산 등..)을 나타내는 값으로, 이 값을 모집단을 전수조사해야만 알수있는 값이다. 그러나 실질적으로 모집단의 크기와 범위가 너무 방대하기에 전수조사를 실지하지 않고 표본조사를 하는데 표본평균,표본분산 등으로 모평균, 모분산등을 추정할수가 있다.

  

## References

- [Naive Bayes classifier] <https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-1%EB%82%98%EC%9D%B4%EB%B8%8C-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EB%B6%84%EB%A5%98-Naive-Bayes-Classification>
- [Naive Bayes classifier] <https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2>
- [Bayesian Estimation] https://datascienceschool.net/02%20mathematics/09.03%20%EB%B2%A0%EC%9D%B4%EC%A6%88%20%EC%B6%94%EC%A0%95%EB%B2%95.html
- [Bayesian Estimation]  <https://sanghyu.tistory.com/10>
- [Bayesian Estimation] <https://lazyis.tistory.com/42>
- [모수] <https://electronicsdo.tistory.com/entry/>
- [조건부 확률] <https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=alwaysneoi&logNo=100148922781>
