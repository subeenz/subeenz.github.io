## Parameter vs. Hyperparameter

### 비교 표

![image-20220408132320919](C:\Users\INNO-2\AppData\Roaming\Typora\typora-user-images\image-20220408132320919.png)

그림 출처 [http://blog.skby.net/%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-hyperparameter/]



## **Parameter**

- 모델 내부(internal)에서 결정되는 변수 -> 데이터를 통해 산출이 가능한 값, 즉 데이터에서 추정되거나 학습됨(사람이 수동으로 설정하지 않음)

```markdown
- 한 클래스에 속해 있는 학생들의 키에 대한 정규분포를 그릴 때 구해진 평균(μ)과 표준편차(σ) 값이 파라미터(parameter)
- 인공 신경망의 가중치
- 서포트 벡터 머신(SVM) 의 서포트 벡터
- 선형 회귀 또는 로지스틱 회귀의 계수

```

* 모델이 예측할 때 필요로 함 -> 모델의 능력을 결정  

* 학습된 모델의 일부로 저장  



## Hyperparameter

* 모델 외부(external)에서 결정되는 변수 -> 데이터를 통해 산출이 가능한 값이 아님

* 정해진 최적의 값이 없고, 휴리스틱(heuristics)한 방법이나 경험 법칙*(rules of thumb)에 의해 최선의 값을 결정

* 모델링 경험으로 현 상황에서의 적절한 값을 찾아내기 위해 Grid Search, Random Search 사용

  #### hyperparameter optimization or hyperparameter tuning

* 딥러닝 뿐만 아니라 모든 기계학습 모델들은 저마다의 하이퍼파라미터를 갖고 있고, 그 값에 따라 성능이 좌지우지한다. 따라서 최적의 성능을 내기 위한 하이퍼파라미터 탐색 방법들이 계속해서 연구되고 있다.

* 하이퍼 파라미터 최적화(Hyperparameter optimization) 또는 튜닝(Tuning)은 학습 알고리즘을위한 최적의 하이퍼 파라미터을 선택하는 문제

* *딥러닝 모델이 최적의 파라미터를 찾아가는 것*. 이 과정이 빠르면 빠를수록 모델의 학습 시간을 줄어들며 그 만큼 원하는 결과를 빨리 얻을 수 있다.

* **Library**

  * **HyperOpt** 

    based on *Bayesian Optimization

    fmin함수로 최적화진행 (다양한 옵션 값 지정 가능)

    https://teddylee777.github.io/thoughts/hyper-opt

  * **Hyperas, kopt, Talos**

    Keras, based on Hyperopt

    * Hyperas : https://github.com/maxpumperla/hyperas

    Keras

    * kopt : https://github.com/Avsecz/kopt=

    keras, TensorFlow, Pytorch

    * Talos : https://github.com/autonomio/talos

  * **Keras Tuner**

    https://www.tensorflow.org/tutorials/keras/keras_tuner?hl=ko

  * **Scikit-Optimize(skopt)**

    NumPy, SciPy, Scikit-Learn

    https://scikit-optimize.github.io/stable/

  * **Spearmint**

    Bayesian

    https://github.com/HIPS/Spearmint

  * **Hyperband**

    official : https://keras.io/api/keras_tuner/tuners/hyperband/

    개인 블로그 : https://iyk2h.tistory.com/143

  * **Sklearn-Deap**

    https://github.com/rsteca/sklearn-deap

  ```markdown
  최적의 Hyperparameter를 찾아가는 방법
  
  - Mini-batch
  - Moentum
  - RMSProp
  - Adam Optimizer
  - Batch Normalization
  - Learning rate decay
  
  설명 블로그 : https://techblog-history-younghunjo1.tistory.com/128 
  ```

  

  

## 단어

* rules of thumb : 이론보다는 실제 경험을 기반으로 하는 대략적인 작업 방법
* Bayesian Theory : https://ddiri01.tistory.com/234
* Learning Rate : 스텝량 (gradient descent algorithm中)



## References

* https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/
* https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tjdudwo93&logNo=221067763334
* https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide
